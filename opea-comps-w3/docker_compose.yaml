# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  vllm-server:
    image: ${REGISTRY:-opea}/vllm:${TAG:-latest}
    container_name: vllm-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:80
    volumes:
      - "${DATA_PATH:-./data}:/data"
      - "${HF_CACHE_DIR:-$HOME/.cache/huggingface}:/root/.cache/huggingface"
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID:-"Qwen/Qwen2.5-0.5B-Instruct"}
      VLLM_TORCH_PROFILER_DIR: "${VLLM_TORCH_PROFILER_DIR:-/mnt}"
      host_ip: ${host_ip}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      VLLM_SKIP_WARMUP: ${VLLM_SKIP_WARMUP:-true}
      trust_remote_code: ${trust_remote_code}
      VLLM_CPU_KVCACHE_SPACE: "50"
      MAX_MODEL_LEN: ${MAX_TOTAL_TOKENS:-2048}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
    command: --model $LLM_MODEL_ID --host 0.0.0.0 --port 80 --trust-remote-code --device cpu
  vllm-openvino:
    image: ${REGISTRY:-opea}/vllm-openvino:${TAG:-latest}
    container_name: vllm-openvino
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:8008
    volumes:
      - "${HF_CACHE_DIR:-$HOME/.cache/huggingface}:/root/.cache/huggingface"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    command: --model ${LLM_MODEL_ID} --host 0.0.0.0 --port 8008 --trust-remote-code
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8008/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 100
  vllm-openvino-arc:
    image: ${REGISTRY:-opea}/vllm-arc:${TAG:-latest}
    container_name: vllm-openvino-arc
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:${LLM_ENDPOINT_PORT:-8008}
    volumes:
      - "${HF_CACHE_DIR:-$HOME/.cache/huggingface}:/root/.cache/huggingface"
      - "./logs:/var/log"
    devices:
      - "/dev/dri:/dev/dri"
    group_add:
      - ${RENDER_GROUP_ID:-110}
    environment:
      - HTTPS_PROXY=${http_proxy}
      - HTTP_PROXY=${https_proxy}
      - LLM_MODEL_ID=${LLM_MODEL_ID}
      - LLM_ENDPOINT_PORT=${LLM_ENDPOINT_PORT}
      - host_ip=${host_ip}
      - trust_remote_code=${trust_remote_code}
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=${VLLM_ALLOW_LONG_MAX_MODEL_LEN}
      - VLLM_OPENVINO_DEVICE=${VLLM_OPENVINO_DEVICE}
      - VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=${VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS}
      - HF_TOKEN=${HF_TOKEN}
    entrypoint: >
      /bin/bash -c "
      echo 'Starting vLLM server with model: ${LLM_MODEL_ID}' &&
      echo 'OpenVINO device: ${VLLM_OPENVINO_DEVICE}' &&
      echo 'Max model length: ${VLLM_ALLOW_LONG_MAX_MODEL_LEN:-2048}' &&
      python3 -m vllm.entrypoints.openai.api_server
      --model ${LLM_MODEL_ID}
      --host 0.0.0.0
      --port ${LLM_ENDPOINT_PORT}
      --max_model_len ${VLLM_ALLOW_LONG_MAX_MODEL_LEN:-2048}
      --trust-remote-code 2>&1 | tee /var/log/vllm.log"
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${LLM_ENDPOINT_PORT}/health || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5

networks:
  default:
    driver: bridge