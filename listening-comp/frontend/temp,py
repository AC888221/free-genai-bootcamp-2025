def render_structured_stage():
    """Render the structured data stage"""
    st.header("Structured Data Processing")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Dialogue Extraction")
        # Placeholder for dialogue processing
        st.info("Dialogue extraction will be implemented here")
        
    with col2:
        st.subheader("Data Structure")
        # Placeholder for structured data view
        st.info("Structured data view will be implemented here")
        
        # Check if transcript data is available in session state
        if 'transcript_data' in st.session_state:
            transcript_data = st.session_state.transcript_data
            transcript = transcript_data['transcript']
            video_id = transcript_data['video_id']
            
            # Initialize the processor
            processor = HSK2TranscriptProcessor()
            
            # Generate prompt
            prompt = processor._generate_prompt(transcript)
            
            # Process with Bedrock
            processed_text = processor._process_with_bedrock(prompt)
            
            if processed_text:
                st.success("Transcript processed successfully!")
                st.text_area("Processed Transcript", processed_text, height=300)
                
                # Save each question individually in their respective section folders
                lines = processed_text.split('\n')
                questions = [line for line in lines if line.strip() and "ï¼š" in line]
                
                output_dir = "backend/data"
                session_questions = []
                for i, question in enumerate(questions, start=1):
                    # Skip sections 1 and 2
                    if i <= 20:
                        continue
                    
                    section = 'qsec3' if 21 <= i <= 30 else 'qsec4'
                    section_dir = os.path.join(output_dir, 'questions', section)
                    if not os.path.exists(section_dir):
                        os.makedirs(section_dir)
                    
                    question_id = processor._generate_id(video_id, i)
                    question_path = os.path.join(section_dir, f"{question_id}.txt")
                    with open(question_path, 'w', encoding='utf-8') as f:
                        f.write(question)
                    st.info(f"Saved question to {question_path}")
                    
                    # Store question in session state
                    session_questions.append({
                        'question_id': question_id,
                        'question': question,
                        'section': section
                    })
                
                # Save questions to session state
                st.session_state.processed_questions = session_questions
                
                # Embed questions
                embedding_model_id = "amazon.titan-embed-image-v1"  # Use the embedding model ID from vector_store.py
                embeddings = embed_questions(session_questions, embedding_model_id)
                
                # Process question files
                for section in ['qsec3', 'qsec4']:
                    section_dir = os.path.join(output_dir, 'questions', section)
                    processed_files = process_question_files(section_dir, embedding_model_id)
                    
                    # Save embeddings
                    embeddings_dir = os.path.join(output_dir, 'embeddings', f'embed_{section}')
                    if not os.path.exists(embeddings_dir):
                        os.makedirs(embeddings_dir)
                    section_embeddings = [emb for emb in embeddings if emb['section'] == section]
                    save_embeddings(section_embeddings, embeddings_dir)
                
                st.success("Questions embedded and saved successfully!")
            else:
                st.error("Failed to process transcript.")
        else:
            st.warning("No transcript data available. Please download a transcript first.")